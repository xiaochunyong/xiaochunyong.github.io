---
layout: post
title:  "Hadoop-2.6.0 & ZooKeeper-3.4.6 & HBase-0.98.8 Cluster Setup"
categories: install tutorial
---
http://www.cnblogs.com/junrong624/p/3564699.html

0. 准备 (作者是在mac下操作,所以有些软件是mac版的)
VMware-Fusion-7.1.0-2314774.dmg
ubuntu-14.04.1-desktop-amd64.iso
jdk-7u60-linux-x64.tar.gz
hadoop-2.6.0.tar.gz
zookeeper-3.4.6.tar.gz
hbase-0.98.8-hadoop2-bin.tar.gz
[百度网盘下载](http://pan.baidu.com)

1.  为了模拟hadoop真实集群环境 我们至少要创建3台虚拟机, 在该教程中我将创建3台虚拟机(为了方便说明, 我称他们为master, slave1, slave2), 系统镜像为(ubuntu-14.04.1-desktop-amd64.iso)
    由于master, slave1, slave2 3台虚拟机需要安装的软件和配置几乎一样, 所以我打算使用VMware创建master虚拟机, 在这台机器上安装并配置好软件, 之后用VMware的clone虚拟机功能复制出slave1,slave2两台虚拟机
1.1 使用VMware 创建master 虚拟机
虚拟机创建的时候设置用户名：hadoop

将hadoop用户添加到sudoers中
sudo vi /etc/sudoers

2. 安装/配置master 虚拟机环境
2.1 (可选) 为了方便, 我会设置文件资源管理器显示隐藏文件
以及
命令行Tab键忽略大小写
echo "set completion-ignore-case on" >> ~/.inputrc

2.2 安装SSH, 配置SSH-KEY 让另外2台机器可以访问 (因为master, slave1, slave2 三台机器需要互相通信)
sudo apt-get install openssh-server

2.3 将安装文件复制到虚拟机桌面
jdk-7u60-linux-x64.tar.gz
hadoop-2.6.0.tar.gz
zookeeper-3.4.6.tar.gz
hbase-0.98.8-hadoop2-bin.tar.gz

2.4 安装JDK, Hadoop, Zookeeper, Hbase
---
    cd ~
    mkdir sdk
    cd sdk
    mv ~/Desktop/*.tar.gz .
    tar -zxvf jdk-7u60-linux-x64.tar.gz
    tar -zxvf hadoop-2.6.0.tar.gz
    tar -zxvf zookeeper-3.4.6.tar.gz
    tar -zxvf hbase-0.98.8-hadoop2-bin.tar.gz

2.5 配置 Hadoop
===
2.5.1 为hadoop分布式文件系统创建数据存储目录
---
    sudo mkdir -p /data/hdfs/name
    sudo mkdir -p /data/hdfs/data
    sudo mkdir -p /data/hdfs/tmp

2.5.2 $HADOOP_HOME/etc/hadoop/hadoop-env.sh
---
    export JAVA_HOME=/home/hadoop/sdk/jdk1.7.0_60

2.5.3 $HADOOP_HOME/etc/hadoop/yarn-env.sh
---
    export JAVA_HOME=/home/hadoop/sdk/jdk1.7.0_60

2.5.4 $HADOOP_HOME/etc/hadoop/slaves
---
    slave1
    slave2

2.5.5 $HADOOP_HOME/etc/hadoop/core-site.xml
---
    <configuration>
      <property>
         <name>fs.defaultFS</name>
         <value>hdfs://master:9000</value>
      </property>
      <property>
         <name>io.file.buffer.size</name>
         <value>131072</value>
      </property>
      <property>
         <name>hadoop.tmp.dir</name>
         <value>file:/data/hdfs/tmp</value>
      </property>
      <property>
         <name>hadoop.proxyuser.hadoop.hosts</name>
         <value>*</value>
      </property>
      <property>
         <name>hadoop.proxyuser.hadoop.groups</name>
         <value>*</value>
      </property>
    </configuration>

2.5.6 $HADOOP_HOME/etc/hadoop/hdfs-site.xml
---
    <configuration>
       <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>master:9001</value>
       </property>
       <property>
          <name>dfs.namenode.name.dir</name>
          <value>file:/data/hdfs/name</value>
       </property>
       <property>
          <name>dfs.datanode.data.dir</name>
          <value>file:/data/hdfs/data</value>
       </property>
       <property>
          <name>dfs.replication</name>
          <value>2</value>
       </property>
       <property>
          <name>dfs.webhdfs.enabled</name>
          <value>true</value>
       </property>
    </configuration>

2.5.7 $HADOOP_HOME/etc/hadoop/mapred-site.xml
---
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
      </property>
    </configuration>

2.5.8 $HADOOP_HOME/etc/hadoop/yarn-site.xml
---
    <configuration>
       <property>
         <name>yarn.nodemanager.aux-services</name>
         <value>mapreduce_shuffle</value>
       </property>
       <property>
         <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
         <value>org.apache.hadoop.mapred.ShuffleHandler</value>
       </property>
       <property>
         <name>yarn.resourcemanager.address</name>
         <value>master:8032</value>
       </property>
       <property>
         <name>yarn.resourcemanager.scheduler.address</name>
         <value>master:8030</value>
       </property>
       <property>
         <name>yarn.resourcemanager.resource-tracker.address</name>
         <value>master:8031</value>
       </property>
       <property>
         <name>yarn.resourcemanager.admin.address</name>
         <value>master:8033</value>
       </property>
       <property>
         <name>yarn.resourcemanager.webapp.address</name>
         <value>master:8088</value>
       </property>
    </configuration>

        2.5.9 集成测试
        ---
            2.5.9.1 格式化文件,格式化文件只能格式化一次
            bin/hadoop namenode -format


2.6 安装配置 Zookeeper
===
2.6.1 zoo.cfg
---
    # The number of milliseconds of each tick
    tickTime=2000
    # The number of ticks that the initial
    # synchronization phase can take
    initLimit=10
    # The number of ticks that can pass between
    # sending a request and getting an acknowledgement
    syncLimit=5
    # the directory where the snapshot is stored.
    # do not use /tmp for storage, /tmp here is just
    # example sakes.
    dataDir=/data/zookeeper
    # the port at which the clients will connect
    clientPort=2181
    # the maximum number of client connections.
    # increase this if you need to handle more clients
    #maxClientCnxns=60
    #
    # Be sure to read the maintenance section of the
    # administrator guide before turning on autopurge.
    #
    # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
    #
    # The number of snapshots to retain in dataDir
    #autopurge.snapRetainCount=3
    # Purge task interval in hours
    # Set to "0" to disable auto purge feature
    #autopurge.purgeInterval=1
    server.1=master:2888:3888
    server.2=slave1:2888:3888
    server.3=slave2:2888:3888

2.6.2 myid
---
cd $ZOOKEEPER_HOME
mkdir data

(集群)
在 master 上执行:
cd $ZOOKEEPER_HOME/data
echo 1 > myid

在 slave1 上执行:
cd $ZOOKEEPER_HOME/data
echo 2 > myid

在 slave2 上执行:
cd $ZOOKEEPER_HOME/data
echo 3 > myid

2.6.3 启动集群
---
    在master, slave1, slave2 上都要执行:
    $ZOOKEEPER_HOME/bin/zkServer.sh start

    随便在一台上执行
    zkCli.sh  -server 192.168.0.21:2181  启动客户端脚本


2.7 安装配置 HBase
===
2.7.1 hbase-env.sh
---
    export JAVA_HOME=/home/hadoop/sdk/jdk1.7.0_60
    export HADOOP_HOME=/home/hadoop/sdk/hadoop-2.6.0
    export HBASE_HOME=/home/hadoop/sdk/hbase-0.98.8-hadoop2
    export PATH=$PATH:$HBASE_HOME/bin
    export HBASE_MANAGES_ZK=false

2.7.2 hbase-site.xml
---
    <configuration>
       <property>
          <name>hbase.rootdir</name>
          <value>hdfs://master:9000/hbase</value>
       </property>
       <property>
          <name>hbase.cluster.distributed</name>
          <value>true</value>
       </property>
       <property>
         <name>hbase.zookeeper.quorum</name>
         <value>master,slave1,slave2</value>
      </property>
      <property>
         <name>zookeeper.session.timeout</name>
         <value>60000</value>
       </property>
       <property>
        <name>hbase.zookeeper.property.clientPort</name>
         <value>2181</value>
       </property>
    </configuration>

2.7.3 regionservers
---
    vi /home/hadoop/hbase-0.98.8-hadoop2/conf/regionservers 写入
    slave1
    slave2


关闭master虚拟机, 使用VMware的Create Linked Clone功能创建slave1和slave2, 并启动三台虚拟机
===
3. 集群配置
===
3.1 hosts
---
使用ifconfig查看三台虚拟机的ip 并且将类似如下内容(IP地址写你自己的)写入到三台虚拟机的hosts文件, 例如:
192.168.101.131 master
192.168.101.132 slave1
192.168.101.133 slave2

3.2 ssh-key
---
使用如下命令在三台虚拟机上生成ssh-key, 然后将三台id_rsa.pub的内容合并，分别写入到三台虚拟机的 ~/.ssh/authorized_keys
    #!/bin/bash
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub
    cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDudCz7htBQ6rz5yexYjxg+hgYtAt5Iygdd3ZveJ+iDVCRQPxVjLl68lTObzfkQNvWt7hYS/v8U3aNxN/wpuSdJicIzgce/GfeQuhNcxTZXx10o8LbiAOanbU5Q9knhhWrs0E+nG1N9DZXdDPzHXYdwWS1vEqp9U7Q4jDGpm2ZDCp++ao6sR+unqa5ZbzpsiEr7+Q1Ky3esD/TPaVrRFmDZqWxySmRG6K7cbOr4lgAqH8e0hVxNbDvdFOmcOrhWD3VhbNMhXMsX4VFTu0ll54z/tE3ntaCm2oDm2LqkwDEqsdAA02dnWdNh9hSwlcKtdQsWq6b31EoeqDzrLHcRB5Wz hadoop@ubuntu
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDYTXKwS1iOSjhXoS/QJbv4AuLmbVAvVpabphk53iLwNxK7mnuf+qDC5UShLmRZL3i7YjPKIFGl9Ug7cMkOmrMuB2J6CuUEbxE0dPZoo5TEkkf1llMhaAVcMif35PEH40QnuhcZaFn3pE+bIid43bJsQLJd6yD3PHNwcoQwuGDmRC86Fgpr5x3JzG9lJqOgeJcpmObTFc369q6MrJOA/IHCByaSOYgRpROm0X8JkEbBH3+GGXFiUvUUBQeoalTt/Ie0uiIJVw+6lEvueU60zwVpdhk3ONrUPYoLmKwZth4sz3U7EDd7UvKXO/JKJhtcrlKPh9z4u3s9fYDPEIlRN8Al hadoop@ubuntu
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0PSNrMQCqbeReEPCcTpPzt2f5y/uXS6drOoW79d+RRb9wa2s8h9uc47LjkSTBLJTIXr88rr23jQTLAta0TLxfo/jC06u2Rk/y9CGkFe8a5apUTWpeb4LesCoLyEDxmrDqY+kjimu/hiUPAQ77UT2sYBrq504z0Clwl2TT/YbVWejBlPbEUFd/HE/XpV8oitaNtcIIXwuA8GzdhyqS5cP0rKyzgm5uq+oa3qH1zugEZ3Re5R0S7shRVuMBmtK9gcLiqmkQhVZhKrRKKinnfofAlSvPkVkB9hwv4K3rJQ6SVVlfb2K+jgVNbyYrOI3GEuflI/D2WLBKuPuZMdbFP/Td hadoop@ubuntu

3.3 myid
---
参考 2.6.1 zoo.cfg 配置文件的末尾
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888

如果server.1=master:2888:3888
则在 master 上执行:
cd $ZOOKEEPER_HOME/data （目录不存在则创建)
echo 1 > myid

在 slave1 上执行:
cd $ZOOKEEPER_HOME/data
echo 2 > myid

在 slave2 上执行:
cd $ZOOKEEPER_HOME/data
echo 3 > myid


3.4 初始化集群
---
3.4.1 Hadoop
3.4.1.1 格式化HDFS (只需在 master 上执行)
bin/hdfs namenode -format

3.4.1.2 启动HDFS & YARN (Map Reduce V2) (只需在 master 上执行)
/sbin/start-dfs.sh
/sbin/start-yarn.sh

3.4.2 ZooKeeper
3.4.2.1 启动ZooKeeper （master,slave1,slave2 都要执行)
zkServer.sh  start

3.4.2.2 启动客户端脚本
zkCli.sh  -server 192.168.0.21:2181

3.4.3 HBase
3.4.3.1 启动HBase (只需在 master 上执行)
bin/start-hbase.sh


/etc/hostname 修改为



					master	       				slave

start-hfs.sh		NameNode                    DataNode
					SecondNameNode

start-yarn.sh       ResourceManager             NodeManager
(mapreduce-v2)

zkServer.sh 		QuorumPeerMain				QuorumPeerMain


start-hbase.sh 		HMaster 					HRegionServer





/etc/hosts

    192.168.101.128 master
    192.168.101.129 slave1
    192.168.101.130 slave2

~/.ssh/authorized_keys

    # master
    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9gy7mInXNpJi6Qjp8FM3WeiLFgrEWkzvBpce3f4mgoue0Z2UXyhPmJGaU3AeH8MbwTLds2OPTewpVBpSfFsq+F6V747bwtcBPlxdS44ewPHJNQf3dzvjU1PUgvRHSsilErkcFyF7/BI5xE8ABN/18eCMq400pE+qUGDan3C07kOtoMUUb/nVsin+icmOmZAAMw3ZdLO3gblXEwBooCywZlc4pX6tcAF3chCS1i/4c4dKVSP6Brrhmor205M/3Z7rLYHHSIZeUKBgjPSwURF2XVEnNyH8xr7IYB6vfl2eai6BStCfFG+GDPlRahFcL+j9bGU2fUD4G0bL1eApDv/o7 ely@ubuntu

    # slave1
    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDB/Hxlr+5Ca77mOvibPFr2od3I2QdIgnvD1cGhZdp3kiyZvsnSEuyFy9+/i3W+zWcOUsV82BjhVS8FS6oByfRDoj+5ypHfJZNxVNsZNibxRq7rlXEm61R9xjNTe2qnQtXerJPywjDsQjbw9Fl1XPZxP/2LSF36rOYEmPWcSRfXLBtj9rXcIMX+YDnvA75ZzqM/3IBJgUdtjRGqhx1xWfW0cRhjuO1edID0Z8wQ18IVF7AYhCfvvrvYiTnSbXQROXlY3Nd0C7Hr6EIUV2XQRXztaUtGROzwj5Okev8eOJoMSTyI0Nopga62Bt7u3DZ+JGA5QNO+0M39Md+d9123HMWT ely@ubuntu

    # slave2
    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7W8QsJ1u6N7G7eOYYfx+fE5EgM1cWpXdxm/uVVLshLAKqML4hNpk+XX7110MIei1M3uzpsV8arwfBANmGRL1bswBuaw/jT8dv97cWVdBfobr4aPW1zcoobRRcuhtXeuuCqGmD2puovnjLhxCDNn4iEILoKSlj5C/B4P5j8cMq9CGoDN/CeO31c8tckq5hVgaKnvcCmW62rB4OIKTPs/atDCGSyG5IYyA8lX+nxR3IhrfClx5CyTARuBW7TgtCiQck3dwc3Ho5AnMORjTeqC8plEGbF1U0MgRqcSw1yrk9RayDuIkRzcUVecyzuLc38tSYmL6W3LbvEi9vWQa/LcdD ely@ubuntu

ssh-key-generate.sh

    #!/bin/bash
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    cat ~/.ssh/id_rsa.pub


集群遇到的问题：
---

问题1: HBase 建表报错
===

通过 bin/hbase shell 进入hbase命令行界面创建表，报错如下:

    ERROR: java.io.IOException: Table Namespace Manager not ready yet, try again later
    at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3101)
    at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1738)
    at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1777)
    at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:38221)
    at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
    at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)

在网上找了些文章，提示看hbase日志
$HBASE_HOME/logs/hbase-{user}-regionserver-ubuntu.log

在日志文件里面看到：

    2014-12-09 05:56:47,227 WARN  [regionserver60020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.
    2014-12-09 05:56:50,227 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=ubuntu,60000,1418132974204 with port=60020, startcode=1418132985081
    2014-12-09 05:56:50,228 WARN  [regionserver60020] regionserver.HRegionServer: error telling master we are up
    com.google.protobuf.ServiceException: java.net.ConnectException: Connection refused

发现log了master=ubuntu (我是用的ubuntu 14.04，系统的hostname为ubuntu,而hosts里面为  127.0.1.1 ubuntu)
解决方法:
sudo vi /etc/hostname 改成slave1 或者 slave2 或者 master (三台虚拟机)
重启三台虚拟机，重新启动hadoop - zookeeper - hbase，解决
http://karo-lee.iteye.com/blog/2056191